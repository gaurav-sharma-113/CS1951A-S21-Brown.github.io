{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from datetime import timedelta, date\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic html parsing\n",
    "The below illustrates the use of beautiful soup for parsing and cleaning the raw content of a webpage. We will use a faculty-favorite, all-too-familiar use case: trying to get a plain text list of the students in one's class, using Banner. It turns out it is easier to do this by writing a web scraper than using the Banner interface itself. #UIGoals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shmo, Joe\n",
      "Shmane, Jane\n",
      "Mouse, Mickey\n",
      "Schmikey, Mickey\n",
      "Man, The\n",
      "Guy, That\n",
      "Duck, Donald\n",
      "Disney, Walt\n",
      "Shmo, Joe\n"
     ]
    }
   ],
   "source": [
    "#html_doc = open(\"../code/Summary_Class_List.html\")\n",
    "req = urllib.request.Request(\"https://cs.brown.edu/people/epavlick/Summary_Class_List.html\")\n",
    "response = urllib.request.urlopen(req)\n",
    "html_doc = response.read()\n",
    "html_dump = BeautifulSoup(html_doc, 'html.parser')\n",
    "#print(\"The raw html:\", html_dump.title)\n",
    "#print(\"The name of the html tag:\", html_dump.title.name)\n",
    "#print(\"The content between the tags:\", html_dump.title.string)\n",
    "#print(\"The content between the tags:\", html_dump.title.text)\n",
    "\n",
    "# list of all the things with the lable \"table\"\n",
    "html_tables = html_dump.find_all('table')\n",
    "#print(len(html_tables))\n",
    "class_list_html = None\n",
    "for table in html_tables:\n",
    "    if table.caption and table.caption.string == \"Summary Class List\":\n",
    "        class_list_html = table\n",
    "#print(class_list_html)\n",
    "\n",
    "rows = class_list_html.find_all('tr')\n",
    "header = rows[0]\n",
    "col_names = [c.string for c in header.find_all('th')]\n",
    "#print(col_names)\n",
    "idx = col_names.index(\"Student Name\")\n",
    "#print(idx)\n",
    "for row in rows[1:]: # first one is header\n",
    "    cols = row.find_all('td')\n",
    "    student_name_col = cols[idx]\n",
    "    print(student_name_col.a.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic web scraping\n",
    "The below illustrates a basic script for crawling a webpage and recursively following links. For this, we will use the Alexa top sites page to get a list of the most popular domains in a given category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bare_bones_request(url):\n",
    "    req = urllib.request.Request(url)\n",
    "    response = urllib.request.urlopen(req)\n",
    "    print(response.read())\n",
    "    \n",
    "def main(TOP_DOMAIN=\"Top\"):\n",
    "    \n",
    "    base_url = 'https://www.alexa.com/topsites/category/'\n",
    "    \n",
    "    # Sometimes you need to give more info in the request, in which case you can use a header\n",
    "    # E.g. this says \"pretend I am coming from firefox\"\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64)'} \n",
    "\n",
    "    # Basic depth first search\n",
    "    stack = [base_url]\n",
    "    while len(stack) > 0:\n",
    "        url = stack.pop() # Python \"pops\" from the end of a list,\n",
    "                          # so append/pop gives stack behavior\n",
    "                          # (idk about you, but this clashes a bit with my mental model,\n",
    "                          # so wanted to comment on it)\n",
    "        try:\n",
    "            req = urllib.request.Request(url, headers=hdr)\n",
    "            response = urllib.request.urlopen(req)\n",
    "        except(UnicodeEncodeError, urllib.error.HTTPError, urllib.error.URLError):\n",
    "            print(\"Error on request: %s\"%url)\n",
    "            continue\n",
    "            \n",
    "        txt = response.read()\n",
    "        doc = BeautifulSoup(txt, 'html.parser')\n",
    "\n",
    "        for item in doc.find_all('a'):\n",
    "            child = item.get('href')\n",
    "            #print(child)\n",
    "            if child:\n",
    "                #print(display_text)\n",
    "                if child.startswith(\"/topsites/category/%s\"%(TOP_DOMAIN)):\n",
    "                    child = child.replace(\"/topsites/category/\", \"\")\n",
    "                    stack.append(base_url+child)\n",
    "                elif child.startswith(\"/siteinfo/\"):\n",
    "                    print('%s\\t%s'%(item.string,\n",
    "                        url.replace(\"https://www.alexa.com/topsites/category/Top/\", \"\")))\n",
    "                    \n",
    "#bare_bones_request('https://www.alexa.com/topsites/category/')\n",
    "main() #TOP_DOMAIN=\"Top/Society\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic API calls\n",
    "The below illustrates the construction of a basic API call, using NY Times API as an example. It shows how to both 1) construct the API call yourself and 2) use a python library to make it a bit cleaner/more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = \"1vReULwjoRj03Qqyb7uIcxlwdSnTJeB0\"\n",
    "\n",
    "base_url = \"https://api.nytimes.com/svc/\"\n",
    "# usual formal of API calls: {endpoint}?key1=value1&key2=value2&...keyn=valuen\n",
    "\n",
    "# search endpoint\n",
    "# search/v2/articlesearch.json?q={query}&fq={filter}\n",
    "\n",
    "call = (base_url + \"/search/v2/articlesearch.json?q=providence&\" + \n",
    "        \"begin_date=20180101&end_date=20181201&api-key=\" + KEY)\n",
    "\n",
    "# Other fun calls to try\n",
    "# share endpoint\n",
    "# mostpopular/v2/shared/{period}/{share_type}.json\n",
    "# most shared on facebook for past 1 day\n",
    "# call = \"https://api.nytimes.com/svc/mostpopular/v2/shared/1/facebook.json?api-key=%s\"\n",
    "\n",
    "# most emailed endpoint\n",
    "# mostpopular/v2/emailed/{period}.json\n",
    "# most emailed for last 7 days\n",
    "# call = \"%s/mostpopular/v2/emailed/7.json?api-key=%s\"%(base_url, KEY)\n",
    "\n",
    "#print(call)\n",
    "req = urllib.request.Request(call)\n",
    "response = urllib.request.urlopen(req)\n",
    "blob = response.read()\n",
    "data = json.loads(blob)\n",
    "#print(data['status'])\n",
    "for article in data['response']['docs']:\n",
    "    print('%s\\n%s\\n'%(article['web_url'], article['snippet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = \"1vReULwjoRj03Qqyb7uIcxlwdSnTJeB0\"\n",
    "\n",
    "base_url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json\"\n",
    "query = \"providence\"\n",
    "start = \"20180101\"\n",
    "end = \"20181201\"\n",
    "\n",
    "params = { 'api-key': KEY,\n",
    "           'q': query, \n",
    "           'begin_date': start, \n",
    "           'end_date': end}\n",
    "\n",
    "response = requests.get(base_url, params=params)\n",
    "response.raise_for_status() # will throw an error if status is not OK\n",
    "data = response.json()\n",
    "\n",
    "print(data)\n",
    "\n",
    "for article in data['response']['docs']:\n",
    "    print('%s\\n%s\\n'%(article['web_url'], article['snippet']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bot?! Who are you calling a bot?\n",
    "The below illustrates some another sometimes useful library which actually launches your browser so that it is less obvious that you are a person. I used this successfully for a while until they started introducing captchas. ::eyeroll:: Surely there are better/more advanced libraries for this now, but this may serve as a useful starting point..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "sdate = \"2019-01-01\"\n",
    "edate = \"2019-01-08\"\n",
    "sy, sm, sd = sdate.split('-')\n",
    "ey, em, ed = edate.split('-')\n",
    "start_date = date(int(sy), int(sm), int(sd))\n",
    "end_date = date(int(ey), int(em), int(ed))\n",
    "\n",
    "search_terms = [\"data science\"]\n",
    "\n",
    "print(\"Search from %s to %s for terms %s\\n\"%(start_date, end_date, str(search_terms)))\n",
    "\n",
    "def main():\n",
    "    chromedriver = '/Users/ellie/Downloads/chromedriver'\n",
    "    os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "    seen = set()\n",
    "    # Iterate through dates\n",
    "    for single_date in daterange(start_date, end_date):\n",
    "        print(single_date)\n",
    "        for term in search_terms:\n",
    "            url = 'https://www.google.com/search?'\n",
    "            values = {'q' : term,\n",
    "              'hl' : 'en',\n",
    "              'gl' : 'us',\n",
    "              'authuser' : '0',\n",
    "              'source' : 'lnt',\n",
    "              'tbs' : ('cdr:1,cd_min:' +\n",
    "                       single_date.strftime(\"%m/%d/%Y\") +\n",
    "                       \",cd_max:\" + single_date.strftime(\"%m/%d/%Y\")),\n",
    "              'tbm' : 'nws',\n",
    "              'start' : '0' }\n",
    "\n",
    "        #print(url + urllib.parse.urlencode(values))\n",
    "        #req = urllib.request.Request(url + urllib.parse.urlencode(values))\n",
    "        #response = urllib.request.urlopen(req) # Throws HTTP Error 403: Forbidden\n",
    "        \n",
    "        try:\n",
    "            driver.get(url + urllib.parse.urlencode(values))\n",
    "        except:\n",
    "            continue\n",
    "        for i,a in enumerate(driver.find_elements_by_tag_name('a')):\n",
    "            try:\n",
    "                link = a.get_attribute('href')\n",
    "                while (link is not None) and (link.startswith('https://ipv4.google.com/sorry/') or link.startswith('https://ipv6.google.com/sorry/')): #captchas\n",
    "                    print(\"Blocked on: %s\\n\"%(str(link)))\n",
    "                    sleep(10)\n",
    "                    link = a.get_attribute('href')\n",
    "                if (link is not None) and (link not in seen) :\n",
    "                    #remove some obvious ads and junk <a> elements\n",
    "                    if ((\"google.com\" not in link) and\n",
    "                        (\"webcache.googleusercontent\" not in link)):\n",
    "                        print(link)\n",
    "                    seen.add(link)\n",
    "            except StaleElementReferenceException:\n",
    "                print(\"Stale element: %s\\n\"%str(a))\n",
    "    driver.close()\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
