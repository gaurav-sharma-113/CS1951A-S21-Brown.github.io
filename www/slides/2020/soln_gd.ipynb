{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see below link for more. my code is somewhat stolen/adapted from here.\n",
    "# https://towardsdatascience.com/linear-regression-simplified-ordinary-least-square-vs-gradient-descent-48145de2cf76\n",
    "# https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/code/ch02/ch02.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random \n",
    "\n",
    "def f(X):\n",
    "    noise = np.random.uniform(-10, 10, X.shape)\n",
    "    return 100 - 2*X + noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data and visualize it\n",
    "reading_level = np.random.uniform(1,16,50)\n",
    "clicks = f(reading_level) \n",
    "\n",
    "x = np.linspace(1,16,50)\n",
    "plt.plot(x, 100 - 2*x, ls=\"--\", color='k')\n",
    "\n",
    "plt.scatter(reading_level, clicks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse(mhat, bhat):\n",
    "    pred = (mhat * reading_level) + bhat\n",
    "    return sum(np.square(clicks - pred))\n",
    "\n",
    "# linear regression the exact way...\n",
    "\n",
    "mu_x = np.mean(reading_level)\n",
    "mu_y = np.mean(clicks)\n",
    "\n",
    "xdiff = reading_level - mu_x\n",
    "ydiff = clicks - mu_y\n",
    "\n",
    "m = sum(np.multiply(xdiff, ydiff)) / sum(np.square(xdiff))\n",
    "print(\"m=%.04f\"%m)\n",
    "\n",
    "b = mu_y - m * mu_x\n",
    "print(\"b=%.04f\"%b)\n",
    "print(\"SSE=%.04f\"%sse(m, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent way\n",
    "\n",
    "def partial_b(mhat, bhat):\n",
    "    pred = (mhat * reading_level) + bhat\n",
    "    grad = np.mean(pred - clicks)\n",
    "    return grad\n",
    "\n",
    "def partial_m(mhat, bhat):\n",
    "    pred = (mhat * reading_level) + bhat\n",
    "    diff = pred - clicks\n",
    "    grad = np.mean(np.multiply(diff, reading_level))\n",
    "    return grad\n",
    "\n",
    "num_iter = 5000\n",
    "alpha = 0.01\n",
    "\n",
    "m0 = 0\n",
    "b0 = 0\n",
    "iters = []\n",
    "for i in range(num_iter):\n",
    "    pb = partial_b(m0, b0)\n",
    "    pm = partial_m(m0, b0)\n",
    "    b0 = b0 - (alpha * pb)\n",
    "    m0 = m0 - (alpha * pm)\n",
    "    iters.append((b0, m0))\n",
    "    \n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.arange(num_iter), [b for b, m in iters])\n",
    "plt.axhline(100, ls=\":\", color='k')\n",
    "plt.title(\"b\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.arange(num_iter), [m for b, m in iters])\n",
    "plt.axhline(-2, ls=\":\", color='k')\n",
    "plt.title(\"m\")\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "for i in [1, 100, 500, 1000, 5000]:\n",
    "    x = np.linspace(1,16,50)\n",
    "    bb, mm = iters[i-1]\n",
    "    plt.plot(x, bb + mm*x, ls=\"--\", color='k')\n",
    "    plt.title(\"i=%s, m=%.02f, b=%.02f\"%(i, mm, bb))\n",
    "    plt.scatter(reading_level, clicks)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic gradient descent way\n",
    "\n",
    "def partial_b(mhat, bhat, x, y):\n",
    "    pred = (mhat * x) + bhat\n",
    "    return pred - y\n",
    "\n",
    "def partial_m(mhat, bhat, x, y):\n",
    "    pred = (mhat * x) + bhat\n",
    "    diff = pred - y\n",
    "    grad = diff * x\n",
    "    return grad\n",
    "\n",
    "data = [(xx, yy) for xx, yy in zip(reading_level, clicks)]\n",
    "\n",
    "num_iter = 1000\n",
    "alpha = 0.001\n",
    "\n",
    "m0 = 0\n",
    "b0 = 0\n",
    "iters = []\n",
    "for i in range(num_iter):\n",
    "    random.shuffle(data)\n",
    "    #if i % 100 == 0:\n",
    "    #    print(\"m=%.02f b=%.02f diff = %s\"%(m0, b0, sse(m0, b0)))\n",
    "    for xx, yy in data:\n",
    "        pb = partial_b(m0, b0, xx, yy)\n",
    "        pm = partial_m(m0, b0, xx, yy)\n",
    "        #print(\"%s\\n%s\\n------\"%(pb, pm))\n",
    "        b0 = b0 - (alpha * pb)\n",
    "        m0 = m0 - (alpha * pm)\n",
    "        iters.append((b0, m0))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.arange(len(iters)), [b for b, m in iters])\n",
    "plt.axhline(100, ls=\":\", color='k')\n",
    "plt.title(\"b\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.arange(len(iters)), [m for b, m in iters])\n",
    "plt.axhline(-2, ls=\":\", color='k')\n",
    "plt.title(\"m\")\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "for i in [1, 1000, 5000, 10000, 20000]:\n",
    "    x = np.linspace(1,16,50)\n",
    "    bb, mm = iters[i-1]\n",
    "    plt.plot(x, bb + mm*x, ls=\"--\", color='k')\n",
    "    plt.title(\"i=%s, m=%.02f, b=%.02f\"%(i, mm, bb))\n",
    "    plt.scatter(reading_level, clicks)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
